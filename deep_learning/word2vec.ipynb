{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I followed along [Aneesh Joshi's blog post](https://medium.com/towards-data-science/learn-word2vec-by-implementing-it-in-tensorflow-45641adaf2ac) on word2vec in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1. read in the data, create word dictionary, created one-hot vectors for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "with open('darksouls_training.txt', 'r') as fh:\n",
    "    training = [sent.replace('.','').replace('\\n', '').lower() for sent in fh.readlines()]\n",
    "# with open('darksouls_test.txt', 'r') as fh:\n",
    "#     test = [sent.replace('.','').replace('\\n', '').lower() for sent in fh.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create vocabulary\n",
    "word_list = []\n",
    "for sent in training:\n",
    "    for word in sent.split(' '):\n",
    "        word_list.append(word)\n",
    "# for sent in test:\n",
    "#     for word in sent.split(' '):\n",
    "#         word_list.append(word)\n",
    "voc = set(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create one-hot vector for each word\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "for ind, word in enumerate(voc):\n",
    "    word2int[word] = ind\n",
    "    int2word[ind] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the sentences\n",
    "sent_train = []\n",
    "for sent in training:\n",
    "    sent_train.append(sent.split(' '))\n",
    "# sent_test = []\n",
    "# for sent in test:\n",
    "#     sent_test.append(sent.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create word pairs\n",
    "data_train = []\n",
    "WINDOW_SIZE = 5\n",
    "for sentence in sent_train:\n",
    "    for ind, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(ind - WINDOW_SIZE, 0) : min(ind + WINDOW_SIZE, len(sentence)) + 1] :\n",
    "            if nb_word != word:\n",
    "                data_train.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert to one-hot\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['postrelease', 'the']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for word_pair in data_train:\n",
    "    x_train.append(to_one_hot(word2int[word_pair[0]], len(voc)))\n",
    "    y_train.append(to_one_hot(word2int[word_pair[1]], len(voc)))\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 2. create tensorflow word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(dtype=tf.float32, shape=(None, len(voc)))\n",
    "y_label = tf.placeholder(dtype=tf.float32, shape=(None, len(voc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hidden layer\n",
    "EMBEDDING_DIM = 5\n",
    "W1 = tf.Variable(tf.random_normal([len(voc), EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "hidden_rep = tf.add(tf.matmul(x, W1), b1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, len(voc), ]))\n",
    "b2 = tf.Variable(tf.random_normal([len(voc)]))\n",
    "pred = tf.nn.softmax(tf.add(tf.matmul(hidden_rep, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('epoch 0: loss is ', 9.6988478)\n",
      "('epoch 100: loss is ', 7.0252614)\n",
      "('epoch 200: loss is ', 6.4158936)\n",
      "('epoch 300: loss is ', 6.1022382)\n",
      "('epoch 400: loss is ', 5.9212928)\n",
      "('epoch 500: loss is ', 5.7993145)\n",
      "('epoch 600: loss is ', 5.7100825)\n",
      "('epoch 700: loss is ', 5.6409087)\n",
      "('epoch 800: loss is ', 5.5849442)\n",
      "('epoch 900: loss is ', 5.5382609)\n",
      "('epoch 1000: loss is ', 5.498414)\n",
      "('epoch 1100: loss is ', 5.4637589)\n",
      "('epoch 1200: loss is ', 5.4331913)\n",
      "('epoch 1300: loss is ', 5.4058681)\n",
      "('epoch 1400: loss is ', 5.3811574)\n",
      "('epoch 1500: loss is ', 5.3585715)\n",
      "('epoch 1600: loss is ', 5.3377142)\n",
      "('epoch 1700: loss is ', 5.3182721)\n",
      "('epoch 1800: loss is ', 5.2999716)\n",
      "('epoch 1900: loss is ', 5.2826042)\n",
      "('epoch 2000: loss is ', 5.2659693)\n",
      "('epoch 2100: loss is ', 5.249928)\n",
      "('epoch 2200: loss is ', 5.2343483)\n",
      "('epoch 2300: loss is ', 5.2191362)\n",
      "('epoch 2400: loss is ', 5.204206)\n",
      "('epoch 2500: loss is ', 5.1894779)\n",
      "('epoch 2600: loss is ', 5.1749139)\n",
      "('epoch 2700: loss is ', 5.1604729)\n",
      "('epoch 2800: loss is ', 5.146121)\n",
      "('epoch 2900: loss is ', 5.1318474)\n",
      "('epoch 3000: loss is ', 5.1176348)\n",
      "('epoch 3100: loss is ', 5.1034875)\n",
      "('epoch 3200: loss is ', 5.0893998)\n",
      "('epoch 3300: loss is ', 5.075387)\n",
      "('epoch 3400: loss is ', 5.0614438)\n",
      "('epoch 3500: loss is ', 5.0476041)\n",
      "('epoch 3600: loss is ', 5.0338593)\n",
      "('epoch 3700: loss is ', 5.0202312)\n",
      "('epoch 3800: loss is ', 5.0067334)\n",
      "('epoch 3900: loss is ', 4.9933681)\n",
      "('epoch 4000: loss is ', 4.9801502)\n",
      "('epoch 4100: loss is ', 4.9670796)\n",
      "('epoch 4200: loss is ', 4.9541721)\n",
      "('epoch 4300: loss is ', 4.9414344)\n",
      "('epoch 4400: loss is ', 4.9288616)\n",
      "('epoch 4500: loss is ', 4.9164505)\n",
      "('epoch 4600: loss is ', 4.9041944)\n",
      "('epoch 4700: loss is ', 4.8921089)\n",
      "('epoch 4800: loss is ', 4.8801861)\n",
      "('epoch 4900: loss is ', 4.8684297)\n",
      "('epoch 5000: loss is ', 4.8568211)\n",
      "('epoch 5100: loss is ', 4.8453736)\n",
      "('epoch 5200: loss is ', 4.8340864)\n",
      "('epoch 5300: loss is ', 4.8229489)\n",
      "('epoch 5400: loss is ', 4.8119636)\n",
      "('epoch 5500: loss is ', 4.8011231)\n",
      "('epoch 5600: loss is ', 4.7904363)\n",
      "('epoch 5700: loss is ', 4.7798896)\n",
      "('epoch 5800: loss is ', 4.7694983)\n",
      "('epoch 5900: loss is ', 4.7592487)\n",
      "('epoch 6000: loss is ', 4.749135)\n",
      "('epoch 6100: loss is ', 4.7391682)\n",
      "('epoch 6200: loss is ', 4.729341)\n",
      "('epoch 6300: loss is ', 4.7196498)\n",
      "('epoch 6400: loss is ', 4.710103)\n",
      "('epoch 6500: loss is ', 4.7006865)\n",
      "('epoch 6600: loss is ', 4.6914148)\n",
      "('epoch 6700: loss is ', 4.6822829)\n",
      "('epoch 6800: loss is ', 4.6732841)\n",
      "('epoch 6900: loss is ', 4.6644254)\n",
      "('epoch 7000: loss is ', 4.6556988)\n",
      "('epoch 7100: loss is ', 4.6471109)\n",
      "('epoch 7200: loss is ', 4.6386619)\n",
      "('epoch 7300: loss is ', 4.6303449)\n",
      "('epoch 7400: loss is ', 4.6221704)\n",
      "('epoch 7500: loss is ', 4.6141214)\n",
      "('epoch 7600: loss is ', 4.6062188)\n",
      "('epoch 7700: loss is ', 4.598444)\n",
      "('epoch 7800: loss is ', 4.590807)\n",
      "('epoch 7900: loss is ', 4.5833068)\n",
      "('epoch 8000: loss is ', 4.5759311)\n",
      "('epoch 8100: loss is ', 4.5686908)\n",
      "('epoch 8200: loss is ', 4.5615811)\n",
      "('epoch 8300: loss is ', 4.554605)\n",
      "('epoch 8400: loss is ', 4.5477576)\n",
      "('epoch 8500: loss is ', 4.5410385)\n",
      "('epoch 8600: loss is ', 4.5344453)\n",
      "('epoch 8700: loss is ', 4.527976)\n",
      "('epoch 8800: loss is ', 4.5216336)\n",
      "('epoch 8900: loss is ', 4.5154109)\n",
      "('epoch 9000: loss is ', 4.5093064)\n",
      "('epoch 9100: loss is ', 4.5033216)\n",
      "('epoch 9200: loss is ', 4.4974566)\n",
      "('epoch 9300: loss is ', 4.4917049)\n",
      "('epoch 9400: loss is ', 4.486064)\n",
      "('epoch 9500: loss is ', 4.4805408)\n",
      "('epoch 9600: loss is ', 4.475121)\n",
      "('epoch 9700: loss is ', 4.4698205)\n",
      "('epoch 9800: loss is ', 4.4646115)\n",
      "('epoch 9900: loss is ', 4.4595108)\n"
     ]
    }
   ],
   "source": [
    "# run the model\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "#loss function\n",
    "cross_entropy_loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(pred),\n",
    "                                                   reduction_indices=1))\n",
    "# training step\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy_loss)\n",
    "# epoch number\n",
    "n_epoch = 10000\n",
    "for epoch in xrange(n_epoch):\n",
    "    sess.run(train_step,\n",
    "             feed_dict={x: x_train, y_label:y_train})\n",
    "    if epoch % 100 == 0:\n",
    "        print('epoch {}: loss is '.format(epoch), sess.run(cross_entropy_loss,\n",
    "                                                           feed_dict={x: x_train, y_label: y_train}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dark_souls_word2vec_model.ckpt'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, 'dark_souls_word2vec_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
