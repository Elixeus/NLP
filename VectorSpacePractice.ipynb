{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ad hoc Information Retrieval System: practice the vector space model\n",
    "Imagine we have a collection of documents, and we would like to make a query\n",
    "to the software to retrieve the document most relevant to the query, what is\n",
    "the technique we should use? One simple model that can be used is called the\n",
    "vector space model. The idea here is to create a hyperspace where each unique\n",
    "word (term) in the collection represents a separate dimension. And each document\n",
    "is represented by a vector composed of the weights (usually correlated with\n",
    "the number of appearances) of each term (dimension). For example, if we have\n",
    "2 recipes in a collections, the fried chicken recipe fc = ['chicken', 'fried',\n",
    "'oil', 'pepper'] and the pouched chicken pc = ['chicken', 'water'], we would\n",
    "have a collection (hyperspace) of 5 dimensions: ['chicken', 'fried', 'oil',\n",
    "'pepper', 'water']. Further assume that in fc, the weight (frequency of word)\n",
    "for each term is [8, 2, 7, 4], and in pc the weights are [6, 5], then the weight\n",
    "represented in our hyperspace are correspondingly fc = [8, 2, 7, 4, 0], pc = [6,\n",
    "0, 0, 0, 5]. Suppose we have a query q = ['fried', 'chicken'] with each term\n",
    "weighting 1, q = [1, 1, 0, 0, 0]. Then in the vector space model, we only need\n",
    "to calculate the cosine similarity between (q, fc) and (q, pc) and compare the\n",
    "results. The more similar the topic, the larger the cosine similarity is. This\n",
    "notebook is a simple implementation of this idea.\n",
    "\n",
    "Footnote: a collection is usually represented by a so-called term-by-document\n",
    "sparse matrix, where the rows resprent the weights of each feature, and the\n",
    "columns represents each document.\n",
    "'''\n",
    "__author__ = 'Xia Wang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a document of 2 files\n",
    "a = 'chicken '*8 + 'fried '*2\n",
    "b = 'chicken '*6\n",
    "documents = (a.strip(), b.strip()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are at least two ways of vectorizing the collection, one is by simple count, the other is use the term frequency * inversed document frequency (to reduce the weight imposed by the very common words but meaningless words such as a, the, and, etc.). Let's start with the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t8\n",
      "  (0, 1)\t2\n",
      "  (1, 0)\t6\n"
     ]
    }
   ],
   "source": [
    "# create a collection matrix (using the count vectorizer)\n",
    "countVectorizer = CountVectorizer(min_df=1)\n",
    "count_matrix = countVectorizer.fit_transform(documents)\n",
    "print count_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a query matrix\n",
    "query = ('chicken', 'fried')\n",
    "q_matrix = countVectorizer.fit_transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.9701425  1.       ]\n",
      "[ 0.24253563  0.        ]\n"
     ]
    }
   ],
   "source": [
    "# compare the cosine similarity\n",
    "results = cosine_similarity(q_matrix, count_matrix)\n",
    "print results[0]\n",
    "print results[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try the second vectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t0.331498529357\n",
      "  (0, 0)\t0.943455735599\n",
      "  (1, 0)\t1.0\n"
     ]
    }
   ],
   "source": [
    "tfidfVectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidfVectorizer.fit_transform(documents)\n",
    "print tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q_matrix_1 = tfidfVectorizer.fit_transform(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94345574  1.        ]\n",
      "[ 0.33149853  0.        ]\n"
     ]
    }
   ],
   "source": [
    "results1 = cosine_similarity(q_matrix_1, tfidf_matrix)\n",
    "print results1[0]\n",
    "print results1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
