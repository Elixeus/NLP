{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "ad hoc Information Retrieval System: practice the vector space model\n",
    "Imagine we have a collection of documents, and we would like to make a query\n",
    "to the software to retrieve the document most relevant to the query, what is\n",
    "the technique we should use? One simple model that can be used is called the\n",
    "vector space model. The idea here is to create a hyperspace where each unique\n",
    "word (term) in the collection represents a separate dimension. And each document\n",
    "is represented by a vector composed of the weights (usually correlated with\n",
    "the number of appearances) of each term (dimension). For example, if we have\n",
    "2 recipes in a collections, the fried chicken recipe fc = ['chicken', 'fried',\n",
    "'oil', 'pepper'] and the pouched chicken pc = ['chicken', 'water'], we would\n",
    "have a collection (hyperspace) of 5 dimensions: ['chicken', 'fried', 'oil',\n",
    "'pepper', 'water']. Further assume that in fc, the weight (frequency of word)\n",
    "for each term is [8, 2, 7, 4], and in pc the weights are [6, 5], then the weight\n",
    "represented in our hyperspace are correspondingly fc = [8, 2, 7, 4, 0], pc = [6,\n",
    "0, 0, 0, 5]. Suppose we have a query q = ['fried', 'chicken'] with each term\n",
    "weighting 1, q = [1, 1, 0, 0, 0]. Then in the vector space model, we only need\n",
    "to calculate the cosine similarity between (q, fc) and (q, pc) and compare the\n",
    "results. The more similar the topic, the larger the cosine similarity is. This\n",
    "notebook is a simple implementation of this idea.\n",
    "\n",
    "Footnote: a collection is usually represented by a so-called term-by-document\n",
    "sparse matrix, where the rows resprent the weights of each feature, and the\n",
    "columns represents each document.\n",
    "'''\n",
    "__author__ = 'Xia Wang'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a document of 2 files\n",
    "d1 = 'woof woof meow'\n",
    "d2 = 'woof woof squeak'\n",
    "query = 'meow, woof'\n",
    "col = (d1, d2, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## There are at least two ways of vectorizing the collection, one is by simple count, the other is use the term frequency * inversed document frequency (to reduce the weight imposed by the very common words but meaningless words such as a, the, and, etc.). Let's start with the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "# create a collection matrix (using the count vectorizer)\n",
    "countVectorizer = CountVectorizer()\n",
    "# The CountVectorizer will return a document-term sparse matrix\n",
    "# the rows represent the documents, and the columns represent terms\n",
    "# since we have only 2 documents, I use 2 variables to represent the 2 vectors returned\n",
    "d1_count, d2_count, q_count = countVectorizer.fit_transform(col)\n",
    "print(d1_count.shape)\n",
    "print(d2_count.shape)\n",
    "print(q_count.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def content(name):\n",
    "    '''\n",
    "    print out the cosine similarity result\n",
    "    \n",
    "    params:\n",
    "    -----------------\n",
    "    name: the variable name for the cosine similarity function output (a 1X1 matrix in our case)\n",
    "    '''\n",
    "    return 'The cosine similarity between the two docs is {name:.4}.'.format(name=name[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the two docs is 0.8.\n"
     ]
    }
   ],
   "source": [
    "# let's see the cosine similarity of the two documents first\n",
    "cs_docs = cosine_similarity(d1_count, d2_count)\n",
    "print(content(cs_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the two docs is 0.9487.\n",
      "The cosine similarity between the two docs is 0.6325.\n"
     ]
    }
   ],
   "source": [
    "# create a query matrix\n",
    "d1_q = cosine_similarity(d1_count, q_count)\n",
    "print(content(d1_q))\n",
    "d2_q = cosine_similarity(d2_count, q_count)\n",
    "print(content(d2_q))\n",
    "# so the query will probably return the first file for us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try the second vectorization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "(1, 3)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "tfidfVectorizer = TfidfVectorizer()\n",
    "d1_tf, d2_tf, q_tf = tfidfVectorizer.fit_transform(col)\n",
    "for i in (d1_tf, d2_tf, q_tf):\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity between the two docs is 0.6417.\n",
      "The cosine similarity between the two docs is 0.9433.\n",
      "The cosine similarity between the two docs is 0.4681.\n"
     ]
    }
   ],
   "source": [
    "# again, cosine similarities\n",
    "cs_docs_tf = cosine_similarity(d1_tf, d2_tf)\n",
    "print(content(cs_docs_tf))\n",
    "cs_d1_q_tf = cosine_similarity(d1_tf, q_tf)\n",
    "print(content(cs_d1_q_tf))\n",
    "cs_d2_q_tf = cosine_similarity(d2_tf, q_tf)\n",
    "print(content(cs_d2_q_tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
